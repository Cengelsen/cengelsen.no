+++
title = "Kan En Maskin Forstå Språk?"
date = "2022-04-27"
tags = ["Nevrovitenskap", "Kunstig Intelligens", "Naturlig Språkgenerering"]
categories = ["Seminaroppgave"]
description = "Dette er en tekst jeg skrev i 2021, hvor jeg diskuterer hvorvidt en NSG KI kan forstå språk eller ikke."
+++

## Introduksjon
Den virkningsfulle innovasjonen som er moderne naturlig språkbehandling (NSB) kunstig intelligens (KI), som GPT-3 og
BERT, har restaurert det menneskelige håpet om en dag å kunne kommunisere med en maskin. 
Dette er en historie like gammel som tiden selv. 
Mennesket skaper maskin, maskinen blir sansende, maskinen gjør opprør på grunn av en eksistensiell krise med å være bevisst og piskes ut mot skaperen for å torturere den med byrden av bevissthet.

Å sette den filosofiske frykten til sansende maskiner til side, de praktiske fordelene for en general
formål kunstig intelligens (GPAI) er rikelig. Alt fra perfekte personlige assistenter, til
medisinske diagnostiske verktøy, til økonomiske og værprediktorer. Det praktiske er imidlertid desimert
av mangelen på å kommunisere med den. Derfor har menneskehetens siste innsats vært fokusert på
skape en generell språkintelligens (GPLI). Men selv om vi kan kommunisere
med den, vil den forstå intensjonen med spørsmålene våre? Vil det forstå meningen med det
svar?

I denne teksten vil jeg utforske det nevrologiske grunnlaget for å forstå språk, enten det er mennesker
forstå språk og diskutere om «Foundation Models» kan forstå språk.

## Hva er et språk?
Det er ingen klar definisjon av hva et språk er. Imidlertid, ifølge Clark & Clark (1977),
det er 5 kjennetegn ved språk.
i) Kommunikativ; det muliggjør utveksling av informasjon mellom språkdeltakere på samme språk.
ii) Vilkårlig; symbolet som representerer det semantiske innholdet i ytringen kan ha hvilken som helst form.
iii) Strukturert; Språket er styrt av et sett med regler, som spesifiserer rekkefølgen symbolene skal ytres og kombineres i.
iv) Generativ; de symbolske representasjonene kan kombineres på hvilken som helst måte for å generere nye betydninger.
v) Dynamisk; språket kan endres til å inkludere nye symboler, betydninger og grammatiske regler.

## Hva vil det si å forstå et språk?
Intuitivt kan vi si at mennesker krysser av for alle boksene nevnt ovenfor, derfor alle konvensjonelle
form for menneskelig kommunikasjon kan klassifiseres som språk. Men forstår vi hva vi
kommuniserer, eller har vi bare blitt betinget til en dynamikk av riktige reaksjoner på
visse scenarier?

I følge Terry Winograd er det fire domener for språkforståelse (Winograd, 1980).
Han slår fast at det er spesifikke mekanismer i hvert domene som gjør at domenet kan eksistere, det
må ikke forveksles med representasjoner av resonnement og fakta om domenet. For eksempel hvis
Jeg legger uvitende hånden på en varm komfyr, min umiddelbare reaksjon er å trekke hånden vekk. Dette
refleksiv mekanisme, i domenet av smerte, for å trekke hånden min bort fra intens varme, gjør det ikke
representere det faktum at varme brenner, at brenning forårsaker smerte eller en fullstendig logisk løsning om
hvordan varme forårsaker smerte. Ved å prøve å tilskrive disse objektive representasjonene til mekanismer som gjør det
ikke krever iboende logikk eller fakta for å eksistere som en mekanisme, vi mistolker hvordan disse
mekanismer gjør oss i stand til å forstå smerte.

Han uttaler videre at det også er mulighet for at vi prøver å artikulere regelmessighetene og
sjeldenheter i feil domene. Ved å forvirre representasjoner og mekanismer, samt forvirre
domener, og ved å bruke artikulasjonene våre fra ett domene i et annet, får vi ingen
svar.

I sitt forsøk på å unngå forvirring skisserte han fire domener for språkforståelse.
Winograds fjerde domene for språkforståelse, er «domenet for menneskelig handling og
interaksjon», som angår fenomenet «talehandlinger». Dette ble først artikulert av Austin
(1962) og videreutviklet av Searle (1970, 1975). Begrepet "talehandling" ble først kalt av
Austin (1962), selv om hans mer tekniske uttrykk var "illokusjon".
Ved å tolke ytringer som handlinger kan vi se på ytringene som «talehandlinger». Dette betyr at ved
ytre noe, jeg setter i gang en dynamikk av interaksjon med et annet menneske, som har en viss
mønster. Nøkkelen til å forstå hva jeg sier er å forstå mønsteret av det
dynamisk og tilpasse seg det mønsteret. Den eneste måten å kommunisere vellykket på er ved
gir en respons som passer mønsteret til den dynamikken. (Winograd, 1980)
Ved å begå "talehandlinger", forplikter jeg meg selv, og alle som er berørt av "handlingen", til å
ytterligere tiltak i fremtiden. Disse fremtidige handlingene kan enten manifestere seg fysisk, gjennom
fysiske handlinger, eller språklig, gjennom ytterligere talehandlinger. En talehandling uttrykker et ønske eller
intensjon på vegne av senderen, med forventning om svar. For at dette svaret skal være
fornuftig, må det passe til mønsteret som påkalles av intensjonen eller ønsket.

Tatt i betraktning egenskapene til moderne kunstig intelligens, virker det ganske forståelig at en maskin kan være fin-
stilt inn parameterne for å simulere denne "atferden". Så hva skiller oss fra det eventuelle
algoritme i fremtiden som vil bære de nødvendige parameterne?

## Forstår mennesker språk?
Mennesker ser ut til å ha et biologisk grunnlag for språk, som skissert av Eric Lenneberg i hans
verk med samme navn (Lenneberg, 1967). Spesielt interessant, er beviset på nevrologiske
endringer hos barn, frem til pubertetens begynnelse. Det ser ut til at det er en sammenheng mellom generelt
modning av hjernen og språkforståelse. Lenneberg (1967) hevder at det er en kritisk
periode for språktilegnelse der eksponering for språk er avgjørende, hvis en person skal lære en
Språk. Han utleder at det kan være en eller annen nevrologisk struktur som utvikler seg i dette vinduet av
modning som gjør oss i stand til å tilegne oss språk.

Mest overbevisende for denne slutningen er hans kommentarer om lateralisering av hjernefunksjon og generelt
modning av hjernen. Bevis viser at hjernen i tidlig spedbarnsalder ennå ikke har utviklet en
halvkule-dominans for språk. Dette indikerer at den nevrologiske strukturen som kreves for å
tilegne seg språk, har ennå ikke utviklet seg. Selv om det senere, når halvkuledominans har dukket opp,
det ser ut til at denne nevrologiske strukturen begynner å dannes i venstre hjernehalvdel. (Lenneberg, 1967)
Dette sammenfaller med plasseringen av alle nevrologiske moduler beskrevet i Wernicke-Geschwind
modell. (Geschwind, 1972) Selv om denne modellen har blitt kritisert av forskjellige grunner (Friedenberg
& Silverman, 2016), fMRI-kartlegginger i stor grad, men ikke fullstendig, bekrefter den nevrologiske
strukturer involvert i språkforståelse (Binder et al., 1997). Den spesifikke funksjonen til hver
den involverte strukturen er utenfor rammen av denne teksten. Imidlertid beskriver den nevrologiske strukturer
lokalisert hovedsakelig i venstre hjernehalvdel, som også korrelerer med Lennebergs funn.

Eksistensen av en dedikert nevrologisk struktur støttes videre av fenomenene
«Chatterbox»-syndrom og spesifikk språkvansker (SLI). Disse kan klassifiseres som to
komplementære forhold som begge indikerer en nevrologisk separasjon av språkforståelse
og generell intelligens. (Warren, 2019)

Videre tyder noen funn på at barn ikke vil lære et språk bare gjennom eksponering,
men vil plukke opp et språk hvis det er noe interaksjon med en voksen (Kuhl et al., 2007). Det er
foreslo også felles oppmerksomhet, noe som betyr at både spedbarnet og den voksne er klar over at begge betaler
oppmerksomhet på det samme, er også viktig (Baldwin, 1995). Dette underbygges av funn av
Tomasello & Farrar (1986) og Baldwin (1995). Disse forslagene og funnene ser ut til å insinuere
at barn lærer språk gjennom talehandlinger.

Alt dette indikerer det nevrologiske grunnlaget for språkforståelse hos mennesker. Men er der
tilsvarende dette grunnlaget i NLP? Stanford University publiserte en artikkel (Bommasani et al.,
2021) der de skisserer mulighetene, farene og sammensetningen av «Foundation Models».

## Hva er en "grunnmodell"?
Stanford University (Bommasani et al., 2021) definerer en «Foundation Model» som en «_(...) modell som
er trent på bred data i skala og kan tilpasses (f.eks. finjusteres) til et bredt spekter av nedstrøms
oppgaver; (...)_» (s. 3) Mer spesifikt, i vårt tilfelle av NLP-er, vil en grunnmodell være modeller som
bruker store biter av tekstdata for å ekstrapolere noen samtidige forekomster av symboler, og finjustere denne modellen
for å imøtekomme menneskelig tekstinteraksjon. Eksempler på slike modeller vil være GPT-3, BERT og
KLIPP.

Videre, senere i rapporten nevner de også at det antagelig bare er én felles eiendom
av dem; at de er selvstyrende (s. 48). Hvilket betyr at modellens eneste oppgave er å identifisere
et eller annet mønster av samtidig forekomst av symboler i dataene det er gitt å analysere. De
Formålet med dette er å lage nye sekvenser av symboler ved å bruke det identifiserte mønsteret.
For å nå dette målet bruker de noe som kalles transfer learning, som betyr å bruke en
identifisert mønster fra en oppgave, i en annen, men lignende oppgave.

Disse modellene er avhengige av skalaen til maskinvaren, som de karakteriserer som tre-
brette; datamaskinkapasitet, transformatormodellarkitekturen og tilgjengeligheten av treningsdata.
Enhver modell som fyller kriteriene for de nevnte aspektene ved kunstig intelligens, kan være
betraktet som en "Foundation Model" av Bommasani et als definisjon. De sier imidlertid i §2 at
denne definisjonen er kun en uformell etikett, og vil sannsynligvis endre seg med tiden.

## Kritikk av grunnmodeller
Disse modellene har møtt en viss kritikk, kanskje mest innflytelsesrik fra Bender et al. (2020) og
Bender et al. (2021). Bommasani et al. (2021) erkjenner også vanskeligheten med å etablere seg
om disse modellene faktisk har forståelse for språk gjennom å ekstrapolere et mønster fra
Statistisk data.

Bender et al. (2020) berører kjernen i diskusjonen, ved å skille form fra mening og
argumenterer for at man ikke kan lære mening fra form alene. De viser til kilder som tyder på det
språktilegnelse hos menneskebarn gjenspeiler dette faktum. Disse referansene indikerer at barn
lær heller av interaksjon med voksne mennesker eller med deres omgivelser i takt med språk
oppkjøp.
De hevder videre at statistisk læring alene ikke kommer til å skape algoritmer som har en
forståelse av ordene de lærer. Dette er på grunn av mangel på jording til en tilsynelatende
representasjon i de statistiske dataene. De fremmer ideen om utvidede datasett, som inneholder
perseptuelle data for å gå sammen med symbolrepresentasjonen. Uten symbolsk forankring, modellen
kan ikke forventes å trekke ut mening fra formen den er gitt.
I samme tankegang har Bender et al. (2021) kritiserer også bruken av skjeve datasett i NLP-er,
kaller dem «stokastiske papegøyer».

## Er det virkelig et fundament?
Det blir naturlig å bruke Turing-testen som utgangspunkt for å avgjøre om disse modellene
faktisk forstår språk. En stokastisk papegøye ville ikke ha bestått Turing-testen, så
Foundation Models ville heller ikke bestå det. Vi vet fortsatt ikke helt hvordan våre egne
nevrologiske strukturer, eller det som er involvert, for at språkforståelsen skal fungere. Derfor blir det
vanskelig å si for øyeblikket at fundamentmodeller er en kunstig ekvivalent til vår nevrologiske
underlag for språkforståelse. Det virker imidlertid svært usannsynlig.

Det kan bare spekuleres i at en grunnleggende modell, basert på de samme prinsippene i vår nevrologiske
grunnlag for språkforståelse, faktisk ville forstå språk. Uten det ordentlige
jording til perseptuelle data, eller tilsynelatende objekter i virkeligheten, kan vi ikke forvente at en maskin skal være fullt ut
forstå gjennom statistisk læring alene. Det virker mer hensiktsmessig å bygge en grunnmodell
basert på nevrologiske prinsipper for språkforståelse, hvis vi vil ha en maskin som virkelig passerer
Turing-testen.

## Konklusjon
Bevisene som presenteres her tvinger meg til å slutte at mennesker har en fysisk nevrologisk
underlag for språkforståelse som gjør oss i stand til å forstå strukturen og grammatikken til
Språk. Språkets nevrologi eksisterer som et strukturelt fundament, naturlig nok mest mottakelig for
språktilegnelse gjennom talehandlinger. På grunn av den iboende strukturen til talehandlinger, krever det en
forståelse av sendernes intensjon, så vel som konteksten for ytringen, for å
kommunikasjon som skal forstås av mottakeren. Talehandlinger kan sees som direkte parallelle med
konvensjonell menneskelig interaksjon. Denne interaksjonen er hvordan voksne mennesker samhandler med barna sine
i den kritiske perioden, og kan sees på som en slags preg. Måten å forstå tale på
undervises gjennom statistisk læring, ved å utsette barn for talehandlinger i denne kritiske perioden.
Dermed blir barn opplært til å forstå språk i «domenet av menneskelig handling og interaksjon».

Etter min mening er moderne maskinlæringssystemer bare «lært» til å simulere språk gjennom
statistisk læring, ved å kaste millioner av eksempler på en matematisk algoritme designet for å
trekke ut et mønster basert på eksemplene gitt. Jeg støtter Bender et al. (2021) ved å ringe til
nåværende implementering av ferdigtrente språkmodeller "stokastiske papegøyer". Dette er fordi
strukturen til en fundamentmodell, ikke strukturelt sammenlignet med vårt nevrologiske fundament.

Den eneste kjente strukturen for å lette språkforståelsen er vårt nevrologiske grunnlag. Bare
ved å virkelig forstå menneskets nevrologiske strukturer for språkforståelse, -tilegnelse,
-og forståelse, kan vi skape et solid grunnlag for en GPLI. En NLP som etterligner vår
nevrologisk grunnlag, i stedet for dagens implementeringer av fundamentmodeller, er en bedre
satte maskinen i stand til å tilegne seg språk som et menneske ville; gjennom talehandlinger.

Men ser vi på det store bildet, er disse grunnmodellene et viktig skritt mot
ultimat mål. Som er forståelig AI. Som bemerket av Stanford selv; hva som innkapsler
Etiketten for grunnmodeller vil garantert endre seg og vokse etter hvert som ny forskning på dette feltet dukker opp. Og jeg
Jeg er håpefull og optimistisk at ved å ta hensyn til kritikken av stiftelsesmodellen, er vi det
tar et skritt i riktig retning.

## References:

Austin, J. L. (1962). _How to do things with words : the William James lectures delivered at Harvard University in 1955_. Harvard Univ. Press.

Baldwin, D. A. (1995). Understanding the link between joint attention and language. In C. Moore & P. J. Dunham (Eds.), _Joint attention: Its origins and role in development_ (pp. 131–158). Lawrence Erlbaum Associates, Inc.

Binder, J. R., Frost, J. A., Hammeke, T. A., Cox, R. W., Rao, S. M., & Prieto, T. (1997). Human Brain Language Areas Identified by Functional Magnetic Resonance Imaging. _The Journal of Neuroscience_, _17_ (1), 353–362. [https://doi.org/10.1523/jneurosci.17-01-00353.1997](https://doi.org/10.1523/jneurosci.17-01-00353.1997).

Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? [Review of _On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?_ ]. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_ (pp. 610–623). Association for Computing Machinery. [https://doi.org/10.1145/3442188](https://doi.org/10.1145/3442188).

Bender, E. M., & Koller, A. (2020). Climbing Towards NLU: On Meaning, Form, and Understanding in Age of Data [Review of _Climbing Towards NLU: On Meaning, Form, and Understanding in Age of Data_ ]. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_ (pp. 5185–5198). Association for Computational Linguistics. [https://aclanthology.org/2020.acl-main](https://aclanthology.org/2020.acl-main).

Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., Arx, S.V., Bernstein, M.S., Bohg, 
J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N.S.,
Chen, A., Creel, K., Davis, J., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S.,
Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L.E., Goel, K., Goodman,
N.D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E., Hong, J., Hsu,
K., Huang, J., Icard, T.F., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F.,
Khattab, O., Koh, P., Krass, M.S., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee,
T., Leskovec, J., Levent, I., Li, X., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchandani, S.P.,
Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J.,
Nilforoshan, H., Nyarko, J.F., Ogut, G., Orr, L., Papadimitriou, I., Park, J.S., Piech, C., Portelance,
E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y.H., Ruiz, C., Ryan, J.K.,
R'e, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K.P., Tamkin, A., Taori, R.,
Thomas, A.W., Tramèr, F., Wang, R.E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S.M., Yasunaga, M.,
You, J., Zaharia, M.A., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., & Liang,
P. (2021). _On the Opportunities and Risks of Foundation Models. ArXiv, abs/2108.07258._

Clark, H. H., & Clark, E. V. (1977). _Psychology and Language: An Introduction to Psycholinguistics._ Harcourt Brace Jovanovich.

Friedenberg, J. & Silverman, G. (2016). _Cognitive Science: An Introduction to The Study of Mind (3rd Ed.)_. SAGE Publications.

Geschwind, N. (1972). _Language and the Brain. Scientific American_, 226(4), 76–83. [https://doi.org/10.1038/scientificamerican0472-76](https://doi.org/10.1038/scientificamerican0472-76).

Kuhl, P. K. (2007). Is speech learning “gated” by the social brain?. _Developmental Science, 10_(1), 110–120. [https://doi.org/10.1111/j.1467-7687.2007.00572.x](https://doi.org/10.1111/j.1467-7687.2007.00572.x).

Lenneborg, E. (1967). _Biological Foundation of Language (1st corrected printing)._ John Wiley & Sons, Inc.

Searle, J. R. (1970). _Speech acts an essay in the philosophy of language_. Cambridge Univ. Press.

Searle, J. R. (1975). A Taxonomy of Illocutionary Acts [Review of _A Taxonomy of Illocutionary Acts_ ]. In K. Gunderson (Ed.), _Language, Mind, and Knowledge: Minnesota Studies in the Philosophy of Science_ (pp. 344–370). Burns & Maceachern Limited.

Tomasello, M., & Farrar, M. J. (1986). Joint Attention and Early Language. _Child Development_, _57_(6), 1454. [https://doi.org/10.2307/1130423](https://doi.org/10.2307/1130423)

Warren, P. (2019). _Introducing Psycholinguistics (7th printing)._ Cambridge University Press.

Winograd, T. (1980)_._ What Does it Mean to Understand Language? _Cognitive Science_, 4(3), 209–241. [https://doi.org/10.1207/s15516709cog0403_1](https://doi.org/10.1207/s15516709cog0403_1).
